####
## Output descriptions:
##

# Treasure Data (http://www.treasure-data.com/) provides cloud based data
# analytics platform, which easily stores and processes data from td-agent.
# FREE plan is also provided.
# @see http://docs.fluentd.org/articles/http-to-td
#
# This section matches events whose tag is td.DATABASE.TABLE
<match td.*.*>
  @type tdlog
  @id output_td
  apikey YOUR_API_KEY

  auto_create_table
  <buffer>
    @type file
    path /var/log/td-agent/buffer/td
  </buffer>

  <secondary>
    @type file
    path /var/log/td-agent/failed_records
  </secondary>
</match>

## match tag=debug.** and dump to console
#<match debug.**>
#  @type stdout
#  @id output_stdout
#</match>

####
## Source descriptions:
##

## built-in TCP input
## @see http://docs.fluentd.org/articles/in_forward
<source>
  @type forward
  @id input_forward
</source>

## built-in UNIX socket input
#<source>
#  type unix
#</source>

# HTTP input
# POST http://localhost:8888/<tag>?json=<json>
# POST http://localhost:8888/td.myapp.login?json={"user"%3A"me"}
# @see http://docs.fluentd.org/articles/in_http
<source>
  @type http
  @id input_http
  port 8888
  add_remote_addr
	body_size_limit 1m
</source>

## live debugging agent
<source>
  @type debug_agent
  @id input_debug_agent
  bind 127.0.0.1
  port 24230
</source>

####
## Examples:
##

## File input
## read apache logs continuously and tags td.apache.access
#<source>
#  @type tail
#  @id input_tail
#  <parse>
#    @type apache2
#  </parse>
#  path /var/log/httpd-access.log
#  tag td.apache.access
#</source>

## File output
## match tag=local.** and write to file
#<match local.**>
#  @type file
#  @id output_file
#  path /var/log/td-agent/access
#</match>

## Forwarding
## match tag=system.** and forward to another td-agent server
#<match system.**>
#  @type forward
#  @id output_system_forward
#
#  <server>
#    host 192.168.0.11
#  </server>
#  # secondary host is optional
#  <secondary>
#    <server>
#      host 192.168.0.12
#    </server>
#  </secondary>
#</match>

## Multiple output
## match tag=td.*.* and output to Treasure Data AND file
#<match td.*.*>
#  @type copy
#  @id output_copy
#  <store>
#    @type tdlog
#    apikey API_KEY
#    auto_create_table
#    <buffer>
#      @type file
#      path /var/log/td-agent/buffer/td
#    </buffer>
#  </store>
#  <store>
#    @type file
#    path /var/log/td-agent/td-%Y-%m-%d/%H.log
#  </store>
#</match>

<filter sodium>
  @type geoip

  # Specify one or more geoip lookup field which has ip address (default: host)
  geoip_lookup_keys REMOTE_ADDR

  # Specify optional geoip database (using bundled GeoLiteCity databse by default)
  # geoip_database    "/path/to/your/GeoIPCity.dat"
  # Specify optional geoip2 database
  # geoip2_database   "/path/to/your/GeoLite2-City.mmdb" (using bundled GeoLite2-City.mmdb by default)
	geoip2_database   "/opt/sodium/location_data/GeoIP2-City.mmdb"
  # Specify backend library (geoip2_c, geoip, geoip2_compat)
  backend_library geoip2_c

  # Set adding field with placeholder (more than one settings are required.)
  <record>
    country         ${country.iso_code["REMOTE_ADDR"]}
    subdivision     ${subdivisions.0.iso_code["REMOTE_ADDR"]}
  </record>

  # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
  skip_adding_null_record  true

  # Set @log_level (default: warn)
  @log_level         info
</filter>

<filter sodium>
  @type geoip

  # Specify one or more geoip lookup field which has ip address (default: host)
	geoip_lookup_keys REMOTE_ADDR

  # Specify optional geoip database (using bundled GeoLiteCity databse by default)
  # geoip_database    "/path/to/your/GeoIPCity.dat"
  # Specify optional geoip2 database
  # geoip2_database   "/path/to/your/GeoLite2-City.mmdb" (using bundled GeoLite2-City.mmdb by default)
	geoip2_database   "/opt/sodium/location_data/GeoIP2-ISP.mmdb"
  # Specify backend library (geoip2_c, geoip, geoip2_compat)
  backend_library geoip2_c

  # Set adding field with placeholder (more than one settings are required.)
  <record>
    isp            ${isp["REMOTE_ADDR"]}
  </record>

  # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
  skip_adding_null_record  false

  # Set @log_level (default: warn)
  @log_level         info
</filter>

<filter sodium>
  @type anonymizer

  # Specify rounding address keys with comma and subnet mask
  <mask network>
    keys  REMOTE_ADDR
    ipv4_mask_bits  24
    ipv6_mask_bits  104
  </mask>
</filter>

<filter sodium>
  @type record_modifier
  remove_keys _dummy_1
  <record>
    _dummy_1 ${if record['country'] == nil || record['country'] == ''; record['country'] = '--'; end; nil}
  </record>
</filter>

<filter sodium>
  @type record_modifier
  remove_keys _dummy_2
  <record>
		_dummy_2 ${if record['subdivision'] == nil || record['subdivision'] == ''; record['subdivision'] = '0'; end; nil}
  </record>
</filter>

<filter sodium>
  @type record_modifier
  remove_keys _dummy_3
  <record>
		_dummy_3 ${if record['isp'] == nil || record['isp'] == ''; record['isp'] = 'unknown'; end; nil}
  </record>
</filter>

<match sodium>
	@type copy
	<store>
	  @type file
		path /var/log/fluent/sodium
		compress gzip
	</store>
	<store>
	  @type http
		endpoint_url    http://localhost:6889/api/sodium
		#http_method     put    # default: post
		serializer      json   # default: form
		#rate_limit_msec 100    # default: 0 = no rate limiting
		raise_on_error  false  # default: true
		#authentication  basic  # default: none
		#username        alice  # default: ''
		#password        bobpop # default: '', secret: true
	</store>
</match>
